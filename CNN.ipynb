{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64b433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import musdb\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Set the path to the dataset directory\n",
    "dataset_dir = \"/Users/shola/Downloads/musdb18/\"\n",
    "duration = 180  # Duration of the songs: those longer than this will be cut and those shorter padded\n",
    "num_songs_to_use = 50\n",
    "batch_size = 20\n",
    "\n",
    "fs = 44100\n",
    "length_songs = duration * fs\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "    print(\"Error: provided path 'dataset_dir' doesn't exist\")\n",
    "\n",
    "# Define the function to extract spectrograms\n",
    "def extract_spectrogram(audio):\n",
    "    spectrogram = librosa.stft(audio, n_fft=2048, hop_length=512)  # Compute the spectrogram using STFT\n",
    "    spectrogram = np.abs(spectrogram)  # Take the absolute value of the complex spectrogram\n",
    "    return librosa.amplitude_to_db(spectrogram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b502afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate spectrograms of mixtures and vocal parts\n",
    "\n",
    "mus = musdb.DB(root=dataset_dir, subsets='train')\n",
    "print(\"Number of downloaded training songs: \" + str(len(mus)))\n",
    "\n",
    "# Iterate over the training dataset directory\n",
    "print(\"Generating spectrograms for each song...\")\n",
    "nb = 0\n",
    "idx = 0\n",
    "for track in mus[0:num_songs_to_use]:\n",
    "    vocals = track.targets['vocals'].audio[:, 0].T\n",
    "    mixture = track.audio[:, 0].T\n",
    "\n",
    "    if len(vocals) > length_songs:\n",
    "        vocals = vocals[0:length_songs]\n",
    "        mixture = mixture[0:length_songs]\n",
    "    else:\n",
    "        # Pad songs with zeros to make sure they all have the same duration (otherwise converting the list to numpy array doesn't work)\n",
    "        vocals = np.append(vocals, np.zeros(length_songs - len(vocals)))\n",
    "        mixture = np.append(mixture, np.zeros(length_songs - len(mixture)))\n",
    "\n",
    "    vocal_spectrogram = extract_spectrogram(vocals)\n",
    "    mixture_spectrogram = extract_spectrogram(mixture)\n",
    "    \n",
    "    if nb == 0: # Init arrays\n",
    "        mixture_spectrograms = np.empty(((len(vocal_spectrogram[0])//batch_size)*num_songs_to_use, len(vocal_spectrogram), batch_size))\n",
    "        vocal_spectrograms = np.empty(((len(vocal_spectrogram[0])//batch_size)*num_songs_to_use, len(vocal_spectrogram), batch_size))\n",
    "        masks = np.empty(((len(vocal_spectrogram[0])//batch_size)*num_songs_to_use, len(vocal_spectrogram)), dtype='bool')\n",
    "    \n",
    "    # Split the spectrogram time samples in batches of size \"batch_size\", dropping those that don't fit\n",
    "    for i in range(len(vocal_spectrogram[0])//batch_size):\n",
    "        vocal_spectrograms[idx, :, :] =  np.array(vocal_spectrogram[:, i*batch_size:(i+1)*batch_size])\n",
    "        mixture_spectrograms[idx, :, :] = np.array(mixture_spectrogram[:, i*batch_size:(i+1)*batch_size])\n",
    "        for f in range(len(vocal_spectrogram)):\n",
    "            non_vocal_spectrogram = mixture_spectrogram[f, i*batch_size:(i+1)*batch_size] - vocal_spectrogram [f, i*batch_size:(i+1)*batch_size]\n",
    "            #print(vocal_spectrogram[f, i*batch_size:(i+1)*batch_size] > non_vocal_spectrogram)\n",
    "            if np.count_nonzero(vocal_spectrogram[f, i*batch_size:(i+1)*batch_size] > 0.2*non_vocal_spectrogram) > batch_size/10:\n",
    "                masks[idx, f] = 1 # If the spectrogram value at frequency f is larger in vocals than non_vocal at least in half of the time samples, consider it is voice\n",
    "            else:\n",
    "                masks[idx, f] = 0\n",
    "        idx += 1\n",
    "\n",
    "    if nb % (num_songs_to_use//10) == 0:\n",
    "        print(str(100*nb/num_songs_to_use) + \"%\")\n",
    "    nb += 1\n",
    "\n",
    "print(len(vocal_spectrograms)) # Total number of 20 samples extracts in the selected songs\n",
    "print(len(vocal_spectrograms[0])) # 1025 frequency bins\n",
    "print(len(vocal_spectrograms[0][0])) # batch_size\n",
    "\n",
    "X_train = np.reshape(mixture_spectrograms, (-1, 1, len(vocal_spectrograms[0]), batch_size))\n",
    "y_train = np.reshape(masks, (-1, len(vocal_spectrograms[0])))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save numpy array in file\n",
    "#np.save(\"vocal_spectrograms.npy\", vocal_spectrograms)\n",
    "#np.save(\"mixture_spectrograms.npy\", mixture_spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fdce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.load(\"vocal_spectrograms.npy\")\n",
    "#y_train = np.load(\"mixture_spectrograms.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a20cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture as a class\n",
    "class SingerIsolationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SingerIsolationNet, self).__init__()\n",
    "        \n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(20500, 20500)\n",
    "        self.fc2 = nn.Linear(20500, 20500)\n",
    "        self.fc3 = nn.Linear(20500, 1025)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the convolutional layers\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        \n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Continue with the modified fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #x = x > 0.5\n",
    "        return x # Convert bool to int\n",
    "\n",
    "# Create an instance of the SingerIsolationNet class\n",
    "model = SingerIsolationNet()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy arrays to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8003ed47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    inputs = X_train\n",
    "    targets = y_train\n",
    "        \n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    epoch_loss = loss.item()\n",
    "    print(\"Epoch Number \" + str(epoch+1) + \": loss = \" + str(epoch_loss))\n",
    "    \n",
    "    \n",
    "#try:\n",
    "    # Save a checkpoint after each epoch\n",
    "#    PATH = f'checkpoint_{epoch}.pt'\n",
    "#    torch.save({\n",
    "#                'epoch': epoch,\n",
    "#                'model_state_dict': model.state_dict(),\n",
    "#                'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                'train_loss': epoch_loss,\n",
    "#                }, \n",
    "#                PATH)\n",
    "#except:\n",
    "#    print(\"Error while saving checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6738ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use the trained neural network to compute the vocals from the song number song_nb in the chosen subset (either 'train' or 'test')\n",
    "mus = musdb.DB(root=dataset_dir, subsets='train')\n",
    "song_nb = 0\n",
    "\n",
    "vocals = mus[song_nb].targets['vocals'].audio[:, 0].T\n",
    "mixture = mus[song_nb].audio[:, 0].T\n",
    "\n",
    "if len(vocals) > length_songs:\n",
    "    vocals = vocals[0:length_songs]\n",
    "    mixture = mixture[0:length_songs]\n",
    "else:\n",
    "    # Pad songs with zeros to make sure they all have the same duration (otherwise converting the list to numpy array doesn't work)\n",
    "    vocals = np.append(vocals, np.zeros(length_songs - len(vocals)))\n",
    "    mixture = np.append(mixture, np.zeros(length_songs - len(mixture)))\n",
    "\n",
    "# Perform singer isolation on a sample mixture\n",
    "sample_vocal_spectrogram = extract_spectrogram(vocals)\n",
    "sample_mixture_spectrogram = extract_spectrogram(mixture)\n",
    "sample_mixture_spectrogram_with_phase = librosa.stft(mixture, n_fft=2048, hop_length=512)\n",
    "song_mask = np.zeros((len(vocal_spectrogram[0])//batch_size, 1025))\n",
    "for i in range(song_nb*len(vocal_spectrogram[0])//batch_size, (song_nb+1)*len(vocal_spectrogram[0])//batch_size):\n",
    "    song_mask[i%(len(vocal_spectrogram[0])//batch_size), :] = masks[i, :]\n",
    "\n",
    "# Split the spectrogram time samples in batches of size \"batch_size\", dropping those that don't fit\n",
    "vocal_spectrograms = np.empty(((len(sample_vocal_spectrogram[0])//batch_size)*len(mus), len(sample_vocal_spectrogram), batch_size))\n",
    "mixture_spectrograms = np.empty(((len(sample_mixture_spectrogram[0])//batch_size)*len(mus), len(sample_mixture_spectrogram), batch_size))\n",
    "mixture_spectrograms_with_phase = np.empty(((len(sample_mixture_spectrogram[0])//batch_size)*len(mus), len(sample_mixture_spectrogram), batch_size), dtype=np.complex_)\n",
    "idx = 0\n",
    "for i in range(len(vocal_spectrogram[0])//batch_size):\n",
    "    vocal_spectrograms[idx, :, :] =  np.array(sample_vocal_spectrogram[:, i*batch_size:(i+1)*batch_size])\n",
    "    mixture_spectrograms[idx, :, :] = np.array(sample_mixture_spectrogram[:, i*batch_size:(i+1)*batch_size])\n",
    "    mixture_spectrograms_with_phase[idx, :, :] = np.array(sample_mixture_spectrogram_with_phase[:, i*batch_size:(i+1)*batch_size], dtype=np.complex_)\n",
    "    idx += 1\n",
    "\n",
    "# Compute the vocals from the mixture using the model\n",
    "predicted_vocal_spectrogram = np.zeros((len(sample_vocal_spectrogram), len(sample_vocal_spectrogram[0])), dtype=np.complex_)\n",
    "reconstructed_mask = np.zeros((idx, len(vocal_spectrograms[0])))\n",
    "for i in range(idx):\n",
    "    sample_mixture_spectrogram = mixture_spectrograms[i, :, :]\n",
    "    sample_vocal_spectrogram = vocal_spectrograms[i, :, :]\n",
    "    \n",
    "    sample_mixture_spectrogram = np.reshape(sample_mixture_spectrogram, (-1, batch_size*len(mixture_spectrograms[0])))\n",
    "\n",
    "    # Convert the sample mixture spectrogram to a PyTorch tensor and add a batch dimension\n",
    "    sample_mixture_spectrogram = torch.from_numpy(sample_mixture_spectrogram).unsqueeze(0).float()\n",
    "\n",
    "    # Pass the sample mixture spectrogram through the model to obtain the frequency mask\n",
    "    with torch.no_grad():\n",
    "        mask = model(sample_mixture_spectrogram).squeeze().numpy()\n",
    "        \n",
    "    predicted_vocal_spectrogram_extract = np.zeros((len(vocal_spectrograms[0]), batch_size), dtype=np.complex_)\n",
    "    for f in range(len(vocal_spectrograms[0])):\n",
    "        #if song_mask[i, f] > 0.5: # If mask is close to 0, the spectrogram stays at 0\n",
    "        if mask[f] > 0.005:\n",
    "            predicted_vocal_spectrogram_extract[f, :] = mixture_spectrograms_with_phase[i, f, :]\n",
    "        else:\n",
    "            predicted_vocal_spectrogram_extract[f, :] = 0.0\n",
    "        reconstructed_mask[i, f] = mask[f] \n",
    "\n",
    "    # Convert the predicted vocal spectrogram to a NumPy array\n",
    "    predicted_vocal_spectrogram_extract = np.reshape(predicted_vocal_spectrogram_extract, (len(vocal_spectrograms[0]), -1))\n",
    "\n",
    "    predicted_vocal_spectrogram[:, i*batch_size:(i+1)*batch_size] = predicted_vocal_spectrogram_extract\n",
    "    \n",
    "# Perform inverse spectrogram to obtain the isolated vocal audio\n",
    "#predicted_vocal_audio = librosa.griffinlim(predicted_vocal_spectrogram)\n",
    "reconstructed_vocal = librosa.istft(predicted_vocal_spectrogram, n_fft=2048, hop_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4576c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Initial vocals extract\")\n",
    "plt.plot(vocals)\n",
    "audio = ipd.Audio(vocals, rate=44100)\n",
    "with open('initial.wav', 'wb') as f:\n",
    "    f.write(audio.data)\n",
    "ipd.Audio(vocals, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0085b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Reconstructed vocals extract\")\n",
    "plt.plot(reconstructed_vocal)\n",
    "audio = ipd.Audio(reconstructed_vocal, rate=44100)\n",
    "with open('reconstructed.wav', 'wb') as f:\n",
    "    f.write(audio.data)\n",
    "ipd.Audio(reconstructed_vocal, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c616cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Initial mixture extract\")\n",
    "plt.plot(mixture)\n",
    "audio = ipd.Audio(mixture, rate=44100)\n",
    "with open('mixture.wav', 'wb') as f:\n",
    "    f.write(audio.data)\n",
    "ipd.Audio(mixture, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c1ec3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predicted_vocal_spectrogram = librosa.amplitude_to_db(np.abs(predicted_vocal_spectrogram)) # Don't need the phase anymore\n",
    "sample_vocal_spectrogram = extract_spectrogram(vocals)\n",
    "print(len(sample_vocal_spectrogram))\n",
    "print(len(sample_vocal_spectrogram[0]))\n",
    "\n",
    "librosa.display.specshow(sample_vocal_spectrogram, sr=44100, hop_length=512, x_axis='time', y_axis='hz')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Spectrogram of original vocals')  # Title of the plot\n",
    "plt.xlabel('Time (s)')  # X-axis label representing time\n",
    "plt.ylabel('Frequency (Hz)')  # Y-axis label representing frequency\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "librosa.display.specshow(predicted_vocal_spectrogram, sr=44100, hop_length=512, x_axis='time', y_axis='hz')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Spectrogram of reconstructed vocals')  # Title of the plot\n",
    "plt.xlabel('Time (s)')  # X-axis label representing time\n",
    "plt.ylabel('Frequency (Hz)')  # Y-axis label representing frequency\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "error_spectrogram = librosa.amplitude_to_db(np.abs(librosa.db_to_amplitude(sample_vocal_spectrogram)-librosa.db_to_amplitude(predicted_vocal_spectrogram)))\n",
    "librosa.display.specshow(error_spectrogram, sr=44100, hop_length=512, x_axis='time', y_axis='hz')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Spectrogram of error = abs(original vocals - reconstructed vocals)')  # Title of the plot\n",
    "plt.xlabel('Time (s)')  # X-axis label representing time\n",
    "plt.ylabel('Frequency (Hz)')  # Y-axis label representing frequency\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "sample_mixture_spectrogram = extract_spectrogram(mixture)\n",
    "librosa.display.specshow(sample_mixture_spectrogram, sr=44100, hop_length=512, x_axis='time', y_axis='hz')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Spectrogram of original mixture')  # Title of the plot\n",
    "plt.xlabel('Time (s)')  # X-axis label representing time\n",
    "plt.ylabel('Frequency (Hz)')  # Y-axis label representing frequency\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "librosa.display.specshow(sample_mixture_spectrogram-sample_vocal_spectrogram, sr=44100, hop_length=512, x_axis='time', y_axis='hz')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Spectrogram of non-vocals (= mixture-vocals)')  # Title of the plot\n",
    "plt.xlabel('Time (s)')  # X-axis label representing time\n",
    "plt.ylabel('Frequency (Hz)')  # Y-axis label representing frequency\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "librosa.display.specshow(sample_vocal_spectrogram > sample_mixture_spectrogram-sample_vocal_spectrogram, sr=44100, hop_length=512, x_axis='time', y_axis='hz')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Result of vocal > non-vocal')  # Title of the plot\n",
    "plt.xlabel('Time (s)')  # X-axis label representing time\n",
    "plt.ylabel('Frequency (Hz)')  # Y-axis label representing frequency\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f0aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target mask\n",
    "song_nb = 0\n",
    "target_mask_song = np.zeros((len(vocal_spectrogram[0])//batch_size, 1025))\n",
    "for i in range(song_nb*len(vocal_spectrogram[0])//batch_size, (song_nb+1)*len(vocal_spectrogram[0])//batch_size):\n",
    "    target_mask_song[i%(len(vocal_spectrogram[0])//batch_size), :] = masks[i, :]\n",
    "plt.pcolormesh(np.transpose(target_mask_song))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Reconstructed mask\n",
    "plt.pcolormesh(np.transpose(reconstructed_mask))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26192643",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1error = np.sum(librosa.db_to_amplitude(error_spectrogram))/np.size(error_spectrogram)\n",
    "print(L1error)\n",
    "MSE = np.sum(librosa.db_to_amplitude(error_spectrogram)**2)/np.size(error_spectrogram)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734b670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94700d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
